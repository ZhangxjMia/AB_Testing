Why should you run a a/b test？
The key to understand what drives your business and make data informed business decision.
To understand the causal relationship and not simply the correlations.

When to run a/b test?
Deciding whether or not to launch a new product and experimental group
To quantify the impact of a feature or product
Compare data with intuition. i.e. to better understand how users respond to certain parts of a product

When not?
No clear comparison between control and experimental group
Emotional changes needs time: Logo/Brand Name
Response data hard to gather
Too long to response


10 Steps Guide to setting up an Experiment
1. Define your goal and form hypotheses
2. Identify control and treatment groups
3. Identify KPIs to measure
4. Identify what data needs to be collected
5. Make sure that appropriate logging is in place to collect all necessary data
6. Determine how small of a difference you would like to detect
7. Determine what fraction of visitors you want to be in the treatment
8. Run a power analysis to decide how much data you need to collect and how long you need to run the test
9. Run the test for AT LEAST this long
10. First time trying something new: run an A/A test simultaneously to check for systematic biases



Case Study 1: Testing a red vs. green button

Step 1: Define goals and form hypotheses
Goal: Quantify the impact a different call-to-action button color (from red to green) has on out core metrics
Hypotheses:
* Compared with a red CTA button, a green CTA button will attract user click more often
* A fraction of these additional clicks will complete the transaction, increasing revenue
* The change in behavior to be most pronounced on mobile (vs. desktop, tablet)
Null Hypothesis:
* Green button will cause no difference on Click Through Rate or other user behavior.

Step 2: Identify treatment & control
Control - Red "Order Now" Button
Treatment - Green "Order Now" Button

Step 3: Identify KPIs to measure
* Revenue
* Purchase Rate: Purchase per visitor
* Click Through Rate: Clicks per visit or Clicks per visitor
Other behavioral metrics:
* Bounce Rate
* Time on Site
* Return Visits
* Engagement with other parts of the website
* Referrals
* etc

Step 4: Identify data to be collected
* User ID (if user is logged in)
* Cookie ID (no logged in)
* What platform the visitor is on
* Page Loads
* Experiment assignment
* if the user sees a button, and which color
* clicks on the button, and which color
* data for other metrics (i.e. engagement behavior)

Step 5: Make sure logging is in place
Smoke test everything!
* Does it work on mobile? How does it work if the user is logged out?
* What happens if you press the “back” button?
* What happens if some other experiment is triggered as well?
* Does the logging interfere with other logging?

Step 6: Determine how small of a difference needs to be detected
Current button CTR: 3%
Successful experiment: 3.3% or more
This means that we would need to collect enough data to measure a 10% increase. (10% depends on company)

Step 7: Determine what fraction of visitors you want to be in the treatment
50/50 splits are the easiest and take the least amount of time to run, but you might not always want to do that.

Step 8: Calculate how much data to collect and how long to run the experiment
Run a power analysis to decide how many data samples to collect depending on your tolerance for:
* Minimum detectable level, aka minimum measurable difference (10% in our case)
* False Negative (Type I error): we see significant result when there isn’t one
    * Falsely reject the null hypothesis
    * Typically, we want a false positive rate of < 5%, alpha = 0.05
    * False positive rate is equivalent to the significant level of a statistic test
* False Positive (Type II error): there is an effect, but we weren’t able to measure it
    * We should reject the null hypothesis, but we did not
    * Typically, we want a false negative rate of < 20%, beta = 0.2
    * The “power” of the test is 1-beta = 0.8

Step 9: Figure out how long to run the test and run the test for AT LEAST that long
For 50/50 split test:
We need at least 53,210 * 2 = 106,420 unique users
If our traffic is about ~100,000 unique users/day, we need to run the test for at least a day.
Note:
If we were to run a 90/10 test, we would need 53,210 * 10 = 532,100 users, the 90% of 532,100 will go to the 90% of the traffic, so that the duration of test will be around 5 times more than the 50/50 split.

Caution: if users are not required to log in, they may be in more than one experiment group! (exclude from test)

Step 10: If this is your first time, run a dummy test (A/A Test)

Summary: Classic button color experiment
* Run a power analysis to determine how long to run test
* Don’t stop the test too early!
* If users do not have to be logged in to be part of the experiment, you may have problems with mixed group users
* To avoid systematic biases, making sure that you have even treatment and control groups


Case Study 2: Testing a New Landing Page

Step 1: Define goals and form hypotheses
Goal: Identify the impact of the new landing page and recommendation page on conversion and user behavior.
Hypotheses:
* Personalized suggestions on homepage will engage visitors who would have otherwise bounced.
* Relevant information on the next page will inspire visitors to make purchases who would not have otherwise.

Step 2: Identify treatment & control
Tip: only one thing should change between treatment and control
Bounce Rate - control vs. treatment 1
Purchase Rate - treatment 1 vs. treatment 2

Step 3: Identify KPIs to measure
* Between Control & Treatment 1: Bounce Rate on home page
* Between Treatment 1 & Treatment 2: Purchase Rate
* Keep track of other metrics too for clues:
    * Click Through Rate
    * Searches
    * Time on site
    * etc

Step 4: Identify data to be collected
* User ID (if user is logged in)
* Cookie ID (no logged in)
* What platform the visitor is on
* Page Loads
* Experiment assignment
* If the user clicks on a recommendation and which one
* Any engagement on the content page

Step 5: Make sure logging is in place
Smoke test everything
* Does it work on mobile? How does it work if the user is logged out?
* What happens if you press the “back” button?
* What happens if some other experiment is triggered as well?
* Does the logging interfere with other logging?

Step 6: Determine how small of a difference needs to detected
Current transactions/visitor: 0.05 (5%)
Successful experiment: 0.051 (5.1%)
Let’s say we would like to detect a 2% increase in transactions

Step 7: Determine what fraction of visitors you want to be in the treatment
Control - 33%
Treatment 1 - 33%
Treatment 2 - 33%

Step 8: Calculate how much data to collect and how long to run the experiment
power.prop.test(p1 = 0.05, p2 = 0.051, sig.level = 0.05, power = 0.8)
-> n = 752,702
n * 3 = 2.25M users are difficult for small company to reach
What if we relaxed significant level to 10%?
-> n = 592,903
n * 3 = 1.78M

Step 9: Figure out how long to run the test and run the test for AT LEAST that long
For alpha = 0.05
We need at least 2.25M users
If our traffic is ~100,000 unique users/day, we need to run the test for 22.5 days
For alpha = 0.1
We need at least 1.78M users
If our traffic is ~100,000 unique users/day, we need to run the test for 18 days

Bonus: Multi-armed bandit approach
Problem: We want to find the best variant, so we are willing to let the experiment run for longer. But we also want to maximize revenue during this experiment period.
Solution: Adjust fraction of (new) users in treatment/control according to which group seems to be doing better.

Summary: Landing page experiment
* Make sure only one thing changes between treatment and control
* Run multi-armed bandit if necessary

A/B Testing Tools
* Google Analytics
* Optimizely
* Unbounce
* Visual Website Optimizer
